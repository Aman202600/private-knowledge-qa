# Detailed Prompts for Project A: Private Knowledge Q&A

## BACKEND PROMPT

```
You are building the backend for a Private Knowledge Q&A web application. This is a FastAPI-based REST API that allows users to upload documents, store them, and ask questions about their content using RAG (Retrieval Augmented Generation).

CORE REQUIREMENTS:
1. Users can upload text documents
2. Documents are chunked and stored with embeddings
3. Users can ask questions and get answers with source citations
4. System health check endpoint for monitoring

TECHNOLOGY STACK:
- Framework: FastAPI (Python 3.10+)
- Database: SQLite (for simplicity, easy to run)
- Vector Store: ChromaDB or FAISS (embedded, no external service)
- LLM Provider: Use environment variable to support multiple (OpenAI, Anthropic, or Groq)
- Embeddings: sentence-transformers (all-MiniLM-L6-v2) - runs locally, no API needed

API ENDPOINTS (MUST MATCH THESE EXACTLY):

1. POST /api/documents/upload
   - Accepts: multipart/form-data with file field
   - Processes: Reads file, chunks text, generates embeddings, stores in DB
   - Returns: { "document_id": "uuid", "filename": "example.txt", "chunks_created": 10, "status": "success" }
   - Error handling: Check file type (.txt only), file size (<5MB), empty files

2. GET /api/documents
   - Returns: List of all uploaded documents
   - Response: { "documents": [{ "id": "uuid", "filename": "example.txt", "uploaded_at": "2024-01-01T10:00:00", "chunk_count": 10 }] }
   - Should be sorted by upload time (newest first)

3. POST /api/query
   - Accepts: { "question": "What is...?", "top_k": 3 }
   - Processes: Embeds question, retrieves relevant chunks, generates answer using LLM
   - Returns: { 
       "answer": "Based on the documents...", 
       "sources": [
         { "document_id": "uuid", "filename": "example.txt", "chunk_text": "relevant text...", "similarity_score": 0.85 }
       ],
       "confidence": "high"
     }
   - Error handling: Empty question, no documents uploaded, LLM API failure

4. GET /api/health
   - Checks: Database connection, vector store status, LLM API connectivity
   - Returns: { 
       "status": "healthy", 
       "database": "connected", 
       "vector_store": "operational", 
       "llm_connection": "connected",
       "llm_provider": "openai",
       "timestamp": "2024-01-01T10:00:00"
     }
   - Should handle graceful degradation if LLM is down but DB is up

5. DELETE /api/documents/{document_id}
   - Deletes document and all its chunks
   - Returns: { "status": "deleted", "document_id": "uuid" }

DATABASE SCHEMA:
- documents table: id (uuid), filename (text), uploaded_at (timestamp), chunk_count (int)
- chunks table: id (uuid), document_id (fk), chunk_text (text), chunk_index (int), created_at (timestamp)
- Vector embeddings stored in ChromaDB/FAISS with metadata linking to chunk_id

CHUNKING STRATEGY:
- Split text into chunks of ~500 characters with 50 character overlap
- Preserve sentence boundaries (don't split mid-sentence)
- Store original chunk text for citation display

LLM INTEGRATION:
- Support multiple providers via environment variable: LLM_PROVIDER (openai/anthropic/groq)
- Use langchain or direct API calls
- Prompt template: "Based on the following context, answer the question. If you cannot answer from the context, say so.\n\nContext:\n{context}\n\nQuestion: {question}\n\nAnswer:"
- Include error handling for API rate limits, timeouts, invalid keys

ERROR HANDLING & EDGE CASES:
- Invalid file uploads (non-text files, empty files, huge files)
- Duplicate filename handling (append timestamp or reject)
- Query with no documents in database
- LLM API failures (return cached/partial response or clear error)
- Malformed requests (missing fields, wrong types)
- Database lock handling for SQLite

ENVIRONMENT VARIABLES (.env.example):
```
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
GROQ_API_KEY=gsk-...
DATABASE_URL=sqlite:///./knowledge_qa.db
VECTOR_STORE_PATH=./chroma_db
MAX_FILE_SIZE_MB=5
CHUNK_SIZE=500
CHUNK_OVERLAP=50
```

CORS CONFIGURATION:
- Allow origins: ["http://localhost:3000", "http://localhost:5173"] for local dev
- Allow credentials: True
- Allow methods: ["GET", "POST", "DELETE"]

LOGGING:
- Log all API requests with timestamp, endpoint, status code
- Log LLM queries and response times
- Log errors with full stack traces

TESTING CONSIDERATIONS:
- Include sample test documents in /test_data folder
- Write at least 3 basic tests: upload document, query, health check
- Mock LLM responses for testing

DOCKER SUPPORT:
- Dockerfile that installs all dependencies
- docker-compose.yml with environment variables
- Should work with: docker-compose up

CODE STRUCTURE:
```
backend/
├── main.py (FastAPI app initialization)
├── models.py (Pydantic models for requests/responses)
├── database.py (SQLite connection and models)
├── vector_store.py (ChromaDB/FAISS wrapper)
├── llm_service.py (LLM provider abstraction)
├── chunking.py (Text chunking logic)
├── requirements.txt
├── Dockerfile
├── docker-compose.yml
├── .env.example
└── tests/
    └── test_api.py
```

README SECTION FOR BACKEND:
Include:
- How to install dependencies: pip install -r requirements.txt
- How to set up .env file
- How to run: uvicorn main:app --reload
- How to run with Docker: docker-compose up
- API endpoint documentation
- Troubleshooting common issues (LLM key errors, port conflicts)

TROUBLESHOOTING BUILT-IN:
1. If LLM API key is invalid, provide clear error message with provider name
2. If vector store fails to initialize, fall back to simple keyword search
3. If database is locked, retry with exponential backoff
4. If file upload fails, return specific reason (size, type, etc.)
5. Add /api/debug endpoint that returns current configuration (without keys)

PERFORMANCE OPTIMIZATIONS:
- Cache embeddings model in memory (load once at startup)
- Use connection pooling for database
- Implement request timeout of 30 seconds
- Batch process multiple chunks for embedding generation

What you should focus on:
1. Get basic upload and query working first
2. Then add health check and proper error handling
3. Test with actual LLM API calls
4. Ensure all endpoints match the specification exactly
5. Make sure CORS is configured properly for frontend connection

Please generate complete, production-ready code with all error handling and edge cases covered.
```

---

## FRONTEND PROMPT

```
You are building the frontend for a Private Knowledge Q&A web application. This is a React-based single-page application that provides an intuitive interface for uploading documents and asking questions about them.

CORE REQUIREMENTS:
1. Upload documents and see confirmation
2. View list of uploaded documents with delete option
3. Ask questions and see answers with source citations
4. View system health status
5. Clean, modern, responsive UI

TECHNOLOGY STACK:
- Framework: React 18+ with Vite
- Styling: Tailwind CSS
- HTTP Client: Axios or fetch API
- State Management: React Context or useState (no Redux needed for this scope)
- Icons: lucide-react or heroicons
- Toast Notifications: react-hot-toast or similar

BACKEND API BASE URL:
- Development: http://localhost:8000
- Use environment variable: VITE_API_URL

API ENDPOINTS TO CONSUME (DO NOT CHANGE THESE):

1. POST /api/documents/upload
2. GET /api/documents
3. POST /api/query
4. GET /api/health
5. DELETE /api/documents/{document_id}

PAGES/COMPONENTS STRUCTURE:

1. HOME PAGE (/)
   - Hero section with brief explanation
   - Two main action buttons: "Upload Documents" and "Ask Questions"
   - Recent activity summary (last 3 documents uploaded, last 3 queries)
   - Clean, welcoming design

2. UPLOAD PAGE (/upload)
   - Drag-and-drop zone for file upload (also click to browse)
   - File validation on client side (.txt only, <5MB)
   - Upload progress indicator
   - Success message with document details after upload
   - List of uploaded documents below with:
     * Filename
     * Upload timestamp (formatted nicely: "2 hours ago")
     * Chunk count
     * Delete button (with confirmation modal)
   - Empty state if no documents uploaded yet

3. QUERY PAGE (/query)
   - Large text input for question (textarea, auto-resize)
   - "Ask Question" button (disabled if input empty or no documents)
   - Loading state while waiting for answer
   - Answer display section:
     * Main answer text (formatted nicely, preserve line breaks)
     * Sources section showing:
       - Document name (clickable to highlight)
       - Relevant text chunk (in a card/box)
       - Similarity score (visual indicator like progress bar)
   - Query history (last 5 queries, collapsible)
   - Clear indication if no documents are uploaded

4. STATUS PAGE (/status)
   - Health check display with visual indicators:
     * Database: Green checkmark or red X
     * Vector Store: Green checkmark or red X
     * LLM Connection: Green checkmark or red X with provider name
   - Last checked timestamp
   - Refresh button to re-check
   - System info: LLM provider being used
   - Color-coded overall status: All green = healthy, any red = degraded

5. NAVIGATION COMPONENT
   - Header/navbar with app name
   - Links to: Home, Upload, Query, Status
   - Active page indicator
   - Responsive mobile menu (hamburger)

ERROR HANDLING & USER FEEDBACK:

1. File Upload Errors:
   - File too large: "File exceeds 5MB limit. Please upload a smaller file."
   - Wrong file type: "Only .txt files are supported. Please upload a text file."
   - Upload failed: "Upload failed: [specific error]. Please try again."
   - Network error: "Cannot connect to server. Please check your connection."

2. Query Errors:
   - Empty question: Disable submit button, show hint "Enter a question first"
   - No documents: Show banner "Upload documents first to ask questions"
   - Query failed: "Failed to get answer: [error]. Please try again."
   - LLM timeout: "Request took too long. Please try again with a simpler question."

3. Delete Confirmation:
   - Modal: "Are you sure you want to delete [filename]? This action cannot be undone."
   - Show loading state while deleting
   - Success message: "[filename] deleted successfully"

4. Network/Server Errors:
   - Detect when backend is down
   - Show friendly error: "Cannot connect to server. Please make sure backend is running."
   - Retry button

EDGE CASES TO HANDLE:

1. No documents uploaded:
   - Show empty state with illustration
   - Call-to-action: "Upload your first document to get started"
   - Disable query functionality with tooltip explaining why

2. Large answer responses:
   - Truncate very long answers with "Read more" toggle
   - Smooth scroll to answer section after response

3. Multiple sources:
   - Display up to 5 sources, "Show more" if more exist
   - Sort sources by similarity score (highest first)

4. Slow API responses:
   - Show loading spinner
   - After 5 seconds, show "Still working..." message
   - After 20 seconds, show option to cancel

5. Mobile responsiveness:
   - Stack layout on mobile
   - Ensure drag-drop works on mobile (or show file picker)
   - Readable text sizes
   - Touch-friendly buttons (min 44x44px)

COMPONENT STRUCTURE:
```
frontend/
├── src/
│   ├── components/
│   │   ├── Navbar.jsx
│   │   ├── FileUploadZone.jsx
│   │   ├── DocumentList.jsx
│   │   ├── DocumentCard.jsx
│   │   ├── QueryInput.jsx
│   │   ├── AnswerDisplay.jsx
│   │   ├── SourceCard.jsx
│   │   ├── HealthIndicator.jsx
│   │   ├── ConfirmModal.jsx
│   │   └── LoadingSpinner.jsx
│   ├── pages/
│   │   ├── Home.jsx
│   │   ├── Upload.jsx
│   │   ├── Query.jsx
│   │   └── Status.jsx
│   ├── services/
│   │   └── api.js (all API calls centralized)
│   ├── context/
│   │   └── AppContext.jsx (optional, for shared state)
│   ├── utils/
│   │   ├── formatDate.js
│   │   └── fileValidation.js
│   ├── App.jsx
│   ├── main.jsx
│   └── index.css (Tailwind imports)
├── public/
│   └── vite.svg (or custom icon)
├── .env.example
├── package.json
├── vite.config.js
├── tailwind.config.js
└── Dockerfile
```

API SERVICE (api.js):
```javascript
// Centralize all API calls here
const API_BASE_URL = import.meta.env.VITE_API_URL || 'http://localhost:8000';

export const uploadDocument = async (file) => {
  const formData = new FormData();
  formData.append('file', file);
  
  const response = await fetch(`${API_BASE_URL}/api/documents/upload`, {
    method: 'POST',
    body: formData,
  });
  
  if (!response.ok) {
    const error = await response.json();
    throw new Error(error.detail || 'Upload failed');
  }
  
  return response.json();
};

export const getDocuments = async () => {
  const response = await fetch(`${API_BASE_URL}/api/documents`);
  if (!response.ok) throw new Error('Failed to fetch documents');
  return response.json();
};

export const askQuestion = async (question, top_k = 3) => {
  const response = await fetch(`${API_BASE_URL}/api/query`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ question, top_k }),
  });
  
  if (!response.ok) {
    const error = await response.json();
    throw new Error(error.detail || 'Query failed');
  }
  
  return response.json();
};

export const deleteDocument = async (documentId) => {
  const response = await fetch(`${API_BASE_URL}/api/documents/${documentId}`, {
    method: 'DELETE',
  });
  
  if (!response.ok) throw new Error('Failed to delete document');
  return response.json();
};

export const getHealth = async () => {
  const response = await fetch(`${API_BASE_URL}/api/health`);
  if (!response.ok) throw new Error('Health check failed');
  return response.json();
};
```

STYLING GUIDELINES:
- Use Tailwind utility classes
- Color scheme: Blue primary (#3B82F6), Gray for text (#374151)
- Consistent spacing: p-4, p-6, p-8 for padding
- Rounded corners: rounded-lg for cards
- Shadows: shadow-md for cards, shadow-lg for modals
- Hover states: hover:bg-blue-600 for buttons
- Transitions: transition-all duration-200

UI/UX BEST PRACTICES:
1. Show loading states for all async operations
2. Disable buttons while operations are in progress
3. Use toast notifications for success/error messages
4. Provide clear feedback for user actions
5. Use skeleton loaders while fetching data
6. Auto-focus on text inputs after page load
7. Keyboard shortcuts: Enter to submit query, Esc to close modals
8. Accessibility: proper ARIA labels, keyboard navigation

ENVIRONMENT VARIABLES (.env.example):
```
VITE_API_URL=http://localhost:8000
```

DOCKER SUPPORT:
- Dockerfile with nginx to serve built React app
- Build with: docker build -t knowledge-qa-frontend .
- Run with: docker run -p 3000:80 knowledge-qa-frontend

README SECTION FOR FRONTEND:
Include:
- How to install: npm install
- How to set up .env file
- How to run dev server: npm run dev
- How to build: npm run build
- How to run with Docker
- Port information (default: 5173 for Vite)
- How to change API base URL

TESTING CONSIDERATIONS:
- Manual testing checklist for each feature
- Test with backend running and stopped
- Test with slow network (throttle in DevTools)
- Test on mobile viewport

TROUBLESHOOTING BUILT-IN:
1. If API connection fails, show clear error with backend URL
2. If CORS error, show message about backend CORS configuration
3. If upload fails, show specific error (file size, type, network)
4. Add developer tools toggle (Shift+Ctrl+D) to show API base URL and connection status

PERFORMANCE:
- Lazy load pages with React.lazy
- Debounce search/filter inputs
- Optimize images (if any)
- Code splitting for better initial load

What you should focus on:
1. Build a clean, working UI first
2. Integrate with backend API endpoints (match them exactly)
3. Add proper error handling and loading states
4. Make it responsive (mobile-friendly)
5. Test all user flows thoroughly
6. Ensure it looks professional and polished

Please generate complete, production-ready code with all features, error handling, and responsive design.
```

---

## INTEGRATION CHECKLIST

Use this to verify frontend and backend work together:

1. ✅ API endpoints match exactly on both sides
2. ✅ CORS is configured in backend for frontend URL
3. ✅ Environment variables are documented in both READMEs
4. ✅ Error responses from backend are handled in frontend
5. ✅ File upload content-type matches (multipart/form-data)
6. ✅ JSON request/response formats match
7. ✅ Both can run with Docker
8. ✅ Health check endpoint works and frontend displays it correctly